<!DOCTYPE HTML>
<html>
	<head>
		<title>Third AAAI Workshop on Privacy-Preserving Artificial Intelligence</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<meta name="keywords" content="PPAI,PPAI22,PPAI2022,PPAI 22,PPAI 2022,AAAI,AAAI22,AAAI 2022,Privacy Preserving AI,Privacy Preserving Artificial Intelligence,AI, Differential Privacy,Privacy,Preserving,Artificial,Intelligence"/>
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">
				<!-- Main -->
				<div id="main">
					<div class="inner">
						<!-- Banner -->
						<section id="banner">
							<div class="content">
								<header>
									<h1>The Third AAAI Workshop on Privacy-Preserving Artificial Intelligence (PPAI-22)</h1>
									<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-calendar3" viewBox="0 0 16 16">
									  <path d="M14 0H2a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2zM1 3.857C1 3.384 1.448 3 2 3h12c.552 0 1 .384 1 .857v10.286c0 .473-.448.857-1 .857H2c-.552 0-1-.384-1-.857V3.857z"/>
									  <path d="M6.5 7a1 1 0 1 0 0-2 1 1 0 0 0 0 2zm3 0a1 1 0 1 0 0-2 1 1 0 0 0 0 2zm3 0a1 1 0 1 0 0-2 1 1 0 0 0 0 2zm-9 3a1 1 0 1 0 0-2 1 1 0 0 0 0 2zm3 0a1 1 0 1 0 0-2 1 1 0 0 0 0 2zm3 0a1 1 0 1 0 0-2 1 1 0 0 0 0 2zm3 0a1 1 0 1 0 0-2 1 1 0 0 0 0 2zm-9 3a1 1 0 1 0 0-2 1 1 0 0 0 0 2zm3 0a1 1 0 1 0 0-2 1 1 0 0 0 0 2zm3 0a1 1 0 1 0 0-2 1 1 0 0 0 0 2z"/>
									</svg>
									February 28, 2022
									<br>
									
                                		<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="mr-1 bi bi-geo-alt" viewBox="0 0 16 16">
                                  		<path d="M12.166 8.94c-.524 1.062-1.234 2.12-1.96 3.07A31.493 31.493 0 0 1 8 14.58a31.481 31.481 0 0 1-2.206-2.57c-.726-.95-1.436-2.008-1.96-3.07C3.304 7.867 3 6.862 3 6a5 5 0 0 1 10 0c0 .862-.305 1.867-.834 2.94zM8 16s6-5.686 6-10A6 6 0 0 0 2 6c0 4.314 6 10 6 10z"/>
                                  		<path d="M8 8a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm0 1a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/>
                                		</svg> 	
                                		<a href="https://aaai-2022.virtualchair.net/index.html">Enter Virtual Venue</a>
                              		 Room: Blue 2
                              		 <!-- https://link.virtualchair.net/aaai-22/eePCFG6B4iJGpnRrBadr/Blue2 -->
								</header>
							</div>
							<span class="center">
								<img class="center" src="images/logo-ppai.png" alt=""/>
						</section>

						<!-- Section -->
						<section>
							<header class="major" id="scope">
								<h2>Scope and Topics</h2>
							</header>
							<p>
							The availability of massive amounts of data, coupled with high-performance cloud computing 
							platforms, has driven significant progress in artificial intelligence and, in particular, 
							machine learning and optimization. It has profoundly impacted several areas, including computer 
							vision, natural language processing, and transportation. However, the use of rich data sets 
							also raises significant privacy concerns: They often reveal personal sensitive information 
							that can be exploited, without the knowledge and/or consent of the involved individuals, for 
							various purposes including monitoring, discrimination, and illegal activities. 
							<br>
							The third AAAI Workshop on Privacy-Preserving Artificial Intelligence (PPAI-22) held at the 
							<a href="https://aaai.org/Conferences/AAAI-22/">Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)</a> 
							builds on the success of previous years 
							<a href="https://www2.isye.gatech.edu/~fferdinando3/cfp/PPAI20/">AAAI PPAI-20</a> and 
							<a href="https://ppai21.github.io/">AAAI PPAI-21</a> 
							to provide a platform for researchers, AI practitioners, and policymakers to discuss technical 
							and societal issues and present solutions related to privacy in AI applications. 
							The workshop will focus on both the theoretical and practical challenges related to the design 
							of privacy-preserving AI systems and algorithms and will have strong multidisciplinary 
							components, including soliciting contributions about policy, legal issues, and societal 
							impact of privacy in AI. 
							</p>
							PPAI-22 will place particular emphasis on: 
							<ol>
								<li>Algorithmic approaches to protect data privacy in the context of learning, 
									optimization, and decision making that raise fundamental challenges for 
								existing technologies.</li> 
								<li>Social issues related to tracking, tracing, and surveillance programs.</li> 
								<li>Algorithms and frameworks to release privacy-preserving benchmarks and data sets.</li> 
							</ol>

							<h3>Topics</h3>
							The workshop organizers invite paper submissions on the following (and related) topics:
							<ul>
								<li>Applications of privacy-preserving AI systems</li>
								<li>Attacks on data privacy</li>
								<li>Differential privacy: theory and applications</li>
								<li>Distributed privacy-preserving algorithms</li>
								<li>Human rights and privacy</li>
								<li>Privacy and Fairness </li>
								<li>Privacy policies and legal issues </li>
								<li>Privacy preserving optimization and machine learning </li>
								<li>Privacy preserving test cases and benchmarks</li>
								<li>Surveillance and societal issues </li>
							</ul>
							<p>
								Finally, the workshop will welcome papers that describe the release of privacy-preserving benchmarks and data sets that can be used by the community to solve fundamental problems of interest, including in machine learning and optimization for health systems and urban networks, to mention but a few examples. 
							</p>

							<h3>Format</h3> 

							<p> 
								The workshop will be a one-day  meeting.  
								The workshop will include a number of technical  sessions, a 
								poster session where presenters can discuss their work, with
								the aim of  further fostering collaborations, multiple invited
								speakers covering crucial challenges for  the field of
								privacy-preserving AI applications, including policy and
								societal impacts, a number of tutorial talks, and will
								conclude with a panel discussion.  
	 						</p>

 							</section>

	

						<section>
							<header class="major" id="dates">
								<h2>Important Dates</h2>
							</header>
							<ul>
							<li><strong>November 23, 2021</strong> – Submission Deadline <font color="#f56a6a">[Extended]</font></li>
							<li><strong>December 5, 2021</strong>  – NeurIPS/AAAI Fast Track Submission Deadline</li>
							<li><strike>December 16, 2021</strike> <strong>December 20, 2021</strong> – Acceptance Notification </li>
							<li><strong>December 31, 2021</strong> – AAAI Early registration deadline</li>
							<li><strong>February 28, 2022</strong> – Workshop Date </li>
							</ul>
						</section>

						<section>
							<header class="major" id="submission">
								<h2>Submission Information</h2>
							</header>

							<p>
							Submission URL:  <a href="https://cmt3.research.microsoft.com/PPAI2022">https://cmt3.research.microsoft.com/PPAI2022</a>
							</p>

							<h3>Submission Types</h3>
							<ul>
								<li><strong>Technical Papers</strong>: 
								Full-length research papers of up to 7 pages (excluding references and appendices) 
								detailing high quality work in progress or work that could potentially be published at 
								a major conference. 
								</li>
								<li><strong>Short Papers</strong>: 
								Position or short papers of up to 4 pages (excluding references and appendices) that 
								describe initial work or the release of privacy-preserving benchmarks and datasets on the 
								topics of interest. 
								</li>
							</ul>

							<h4>NeurIPS/AAAI Fast Track (Rejected AAAI papers)</h4>
							<p>
							Rejected NeurIPS/AAAI papers with *average* scores of at least 4.5 may be asubmitted directly to PPAI 
							along with previous reviews. These submissions may go through a light review process or 
							accepted if the provided reviews are judged to meet the workshop standard. 
							</p>

							<p>	
							All papers must be submitted in PDF format, using the <a href="https://www.aaai.org/Publications/Templates/AuthorKit22.zip">AAAI-22 author kit</a>. 
							Submissions should include the name(s), affiliations, and email addresses of all authors. 
							<br>
							Submissions will be refereed on the basis of technical quality, novelty, significance, and 
							clarity. Each submission will be thoroughly reviewed by at least two program committee members.
							<br>
							Submissions of papers rejected from the AAAI 2022 technical program are welcomed. 
							<!-- Papers will be selected for oral and/or poster presentation at the workshop.  -->
							</p>

							<p>
							For questions about the submission process, contact the workshop <a href="#contact">chairs</a>.
							</p>
						</section>

						<section>
			                            <header class="major" id="registration">
							<h2>Registration</h2>
						    </header>
						    <p>Registration in each workshop is required by all active participants, and is also open to all interested individuals.
						    <strong>Early registration deadline is on December 31th</strong>. For more information please refer to <a href="https://aaai.org/Conferences/AAAI-22/ws22/">AAAI-22 Workshop page</a>.</p>
						</section>


					<section>
							<header class="major" id="program">
								<h2>Program</h2>
							</header>


							All times are in Eastern Standard Time (UTC-5).</br><br>

							<strong>Invited talks, Tutorials, and Panel discussion</strong>: 
							Will be live streamed. Access via 
							<a href="https://virtualchair.zoom.us/j/83333462772?pwd=elM2UkJqVXAyNWpRbVZHbmhTMzVIQT09#success">Virtual Chair</a>.

							</br>
							<strong>Spotlights and Poster Talks</strong>: Are pre-recorded and 
							will be accessible at any time. <!-- (click on the play button next to the associated paper).  -->
							There will be additional Q&A and discussion at the poster sessions.
							<br>
							<strong>Poster sessions</strong>: are hosted on <a href="https://aaai-2022.virtualchair.net/venue.html">Virtual Chair (Room Blue 2)</a>. 
							<br><br>

							<div class="table-wrapper">
								<table>
									<thead>
										<tr>
											<th colspan="1">Time</th>
											<th>Talk / Presenter </th>
											<th></th>
										</tr>
									</thead>
									<tbody>
										<tr><td>09:50</td>	<td>Introductory remarks</td> <td></td>		</tr>
										<tr><td>10:00</td>	<td><a href="#damien">Invited Talk</a>: "A bottom-up approach to making differential privacy ubiquitous" by <em>Damien Desfontaines</em></td> <td style="text-align: center;"><a href="https://virtualchair.zoom.us/j/83333462772?pwd=elM2UkJqVXAyNWpRbVZHbmhTMzVIQT09#success" target="_blank" style="border-bottom: none"><img src="images/live-stream.png" width="20"></a></td>		</tr>
										<tr height="8px">
										<td colspan="3">Session chair: <em>Ferdinando Fioretto</em></td></tr>
										<tr><td>10:45</td>	<td><a href="#sp1">Spotlight Talk</a>: Differential privacy and robust statistics in high dimensions</td>							<td style="text-align: center;"><a href="https://web.ecs.syr.edu/~ffiorett/cfp/PPAI22/5.mp4" alt="Video" style="border-bottom: none"><img src="images/play.png" width="20" style="vertical-align:middle"></a></td>		</tr>
										<tr><td>11:00</td>	<td><a href="#sp2">Spotlight Talk</a>: Element Level Differential Privacy: The Right Granularity of Privacy</td>					<td style="text-align: center;"><a href="https://web.ecs.syr.edu/~ffiorett/cfp/PPAI22/11.mp4" alt="Video" style="border-bottom: none"><img src="images/play.png" width="20" style="vertical-align:middle"></a></td>		</tr>
										<tr><td>11:15</td>  <td colspan="2">Break</td></tr>

										<td colspan="3">Session chair: <em>Fatemeh Mireshghallah</em></td></tr>
										<tr><td>11:30</td>	<td><a href="#sp3">Spotlight Talk</a>: A Fairness Analysis on Private Aggregation of Teacher Ensembles</td>							<td style="text-align: center;"><a href="https://web.ecs.syr.edu/~ffiorett/cfp/PPAI22/22.mp4" alt="Video" style="border-bottom: none"><img src="images/play.png" width="20" style="vertical-align:middle"></a></td>		</tr>
										<tr><td>11:45</td>	<td><a href="#sp4">Spotlight Talk</a>: Benchmarking Differentially Private Synthetic Data Generation Algorithms</td>				<td style="text-align: center;"><a href="https://web.ecs.syr.edu/~ffiorett/cfp/PPAI22/26.mp4" alt="Video" style="border-bottom: none"><img src="images/play.png" width="20" style="vertical-align:middle"></a></td>		</tr>
										<tr><td>12:00</td>	<td><a href="#ilya">Tutorial</a>: "Differentially Private Deep Learning, Theory, Attacks, and PyTorch Implementation" by <em>Ilya Mironov, Alexandre Sablayrolles, and Igor Shilov</em></td> <td style="text-align: center;"><a href="https://virtualchair.zoom.us/j/83333462772?pwd=elM2UkJqVXAyNWpRbVZHbmhTMzVIQT09#success" target="_blank" style="border-bottom: none"><img src="images/live-stream.png" width="20"></a></td>		</tr>


										<tr><td>13:45</td>	<td>Flash Poster Presentations</td> 												<td style="text-align: center; white-space: nowrap;"> 	</td>	</tr>
										<tr><td>14:00</td>  <td colspan="2">Poster Session (on <a href="https://aaai-2022.virtualchair.net/index.html">VirtualChair</a> Room Blue 2)</td></tr>

										<tr><td>15:00</td>	<td><a href="#adam">Invited Talk:</a> "When is Memorization Necessary for Machine Learning?" by <em>Adam Smith</em></td> <td style="text-align: center;"><a href="https://virtualchair.zoom.us/j/83333462772?pwd=elM2UkJqVXAyNWpRbVZHbmhTMzVIQT09#success" target="_blank" style="border-bottom: none"><img src="images/live-stream.png" width="20"></a></td>		</tr>										
										
										<tr height="8px"><td colspan="3">Session chair: <em>Xi He</em></td></tr>
										<tr><td>15:45</td>	<td><a href="#sp5">Spotlight Talk</a>: Calibration with Privacy in Peer Review: A Theoretical Study</td>						<td style="text-align: center;"><a href="https://web.ecs.syr.edu/~ffiorett/cfp/PPAI22/27.mp4" alt="Video" style="border-bottom: none"><img src="images/play.png" width="20" style="vertical-align:middle"></a></td></tr>
										<tr><td>16:00</td>	<td><a href="#sp6">Spotlight Talk</a>: APRIL: Finding the Achilles’ Heel on Privacy for Vision Transformers</td>				<td style="text-align: center;"><a href="https://web.ecs.syr.edu/~ffiorett/cfp/PPAI22/29.mp4" alt="Video" style="border-bottom: none"><img src="images/play.png" width="20" style="vertical-align:middle"></a></td>		</tr>
										<tr><td>16:15</td>  <td colspan="2">Break</td></tr>

										<tr><td>16:30</td>	<td><a href="#claire">Invited Talk:</a> "Personal Privacy and the Public Good" by <em>Claire McKay Bowen</em>: Personal Privacy and the Public Good</td>			<td style="text-align: center;"><a href="https://virtualchair.zoom.us/j/83333462772?pwd=elM2UkJqVXAyNWpRbVZHbmhTMzVIQT09#success" target="_blank" style="border-bottom: none"><img src="images/live-stream.png" width="20"></a></td></tr>
										<tr><td>17:30</td>  <td><a href="#panel">Panel Discussion</a>: Differential Privacy and its disparate impacts.</td><td style="text-align: center;"><a href="https://aaai-2022.virtualchair.net/venue.html" target="_blank" style="border-bottom: none"><img src="images/live-stream.png" width="20"></a></td></tr>
										<tr><td>18:20</td>  <td colspan="2">Concluding Remarks and Poster Session (on <a href="https://aaai-2022.virtualchair.net/venue.html">VirtualChair</a> Room Blue 2)</td></tr>
									</tbody>
								</table>
							</div>


							<header class="major" id="accepted_papers">
								<h3>Accepted Papers</h3>
							</header>

								<h5 id="spotlight">Spotlight Presentations</h5>
								<ul>
									<li id="sp1"><font color="#f56a6a">Differential privacy and robust statistics in high dimensions</font> 
										<a href="files/5.pdf" alt="ArXiv Link" style="border-bottom: none"><img src="images/pdf-icon.png" width="20" style="vertical-align:middle"></a>
										<a href="https://web.ecs.syr.edu/~ffiorett/cfp/PPAI22/5.mp4" alt="Video" style="border-bottom: none"><img src="images/play.png" width="20" style="vertical-align:middle"></a>
										<a href="https://arxiv.org/abs/2111.06578" alt="ArXiv Link" style="border-bottom: none"><strong>[ArXiv]</strong></a>
										<br>
										<em>Xiyang Liu (University of Washington); Weihao Kong (University of Washington); Sewoong Oh (University of Washington)</em><br>
										<!-- https://arxiv.org/pdf/2006.16385.pdf -->
									</li>
									<li id="sp2"><font color="#f56a6a">Element Level Differential Privacy: The Right Granularity of Privacy</font>
										<a href="files/11.pdf" alt="ArXiv Link" style="border-bottom: none"><img src="images/pdf-icon.png" width="20" style="vertical-align:middle"></a>
										<a href="https://web.ecs.syr.edu/~ffiorett/cfp/PPAI22/11.mp4" alt="Video" style="border-bottom: none"><img src="images/play.png" width="20" style="vertical-align:middle"></a>
										<a href="https://arxiv.org/abs/1912.04042" alt="ArXiv Link" style="border-bottom: none"><strong>[ArXiv]</strong></a>
										<br>
										<em>Hilal Asi (Stanford University); John Duchi (Stanford University); Omid Javidbakht (Apple)</em>
									</li>
									<li  id="sp3"><font color="#f56a6a">A Fairness Analysis on Private Aggregation of Teacher Ensembles</font>
										<a href="files/22.pdf" alt="ArXiv Link" style="border-bottom: none"><img src="images/pdf-icon.png" width="20" style="vertical-align:middle"></a>
										<a href="https://web.ecs.syr.edu/~ffiorett/cfp/PPAI22/22.mp4" alt="Video" style="border-bottom: none"><img src="images/play.png" width="20" style="vertical-align:middle"></a>
										<a href="https://arxiv.org/abs/2109.08630" alt="ArXiv Link" style="border-bottom: none"><strong>[ArXiv]</strong></a>
										<br>
										<em>Cuong Tran (Syracuse University); My H Dinh (Syracuse University); Kyle Beiter (Syracuse University); Ferdinando Fioretto (Syracuse University)</em>
										
									</li>
									<li  id="sp4"><font color="#f56a6a">Benchmarking Differentially Private Synthetic Data Generation Algorithms</font>
										<a href="files/26.pdf" alt="ArXiv Link" style="border-bottom: none"><img src="images/pdf-icon.png" width="20" style="vertical-align:middle"></a>
										<a href="https://web.ecs.syr.edu/~ffiorett/cfp/PPAI22/26.mp4" alt="Video" style="border-bottom: none"><img src="images/play.png" width="20" style="vertical-align:middle"></a>
										<a href="https://arxiv.org/abs/2112.09238" alt="ArXiv Link" style="border-bottom: none"><strong>[ArXiv]</strong></a>
										<br>
										<em>Yuchao Tao (Duke University); Ryan McKenna (University of Massachusetts, Amherst); Michael Hay (Tumult Labs); Ashwin Machanavajjhala (Tumult Labs); Gerome Miklau (Tumult Labs)</em>
									</li>
									<li  id="sp5"><font color="#f56a6a">Calibration with Privacy in Peer Review: A Theoretical Study</font>
										<a href="files/27.pdf" alt="ArXiv Link" style="border-bottom: none"><img src="images/pdf-icon.png" width="20" style="vertical-align:middle"></a>
										<a href="https://web.ecs.syr.edu/~ffiorett/cfp/PPAI22/27.mp4" alt="Video" style="border-bottom: none"><img src="images/play.png" width="20" style="vertical-align:middle"></a>
										<a href="https://arxiv.org/abs/2201.11308" alt="ArXiv Link" style="border-bottom: none"><strong>[ArXiv]</strong></a>
										<br>
										<em>Wenxin Ding (University of Chicago); Gautam Kamath (University of Waterloo); Weina Wang (CMU); Nihar Shah (CMU)</em>
									</li>
									<li id="sp6"><font color="#f56a6a">APRIL: Finding the Achilles’ Heel on Privacy for Vision Transformers</font>
										<a href="files/29.pdf" alt="ArXiv Link" style="border-bottom: none"><img src="images/pdf-icon.png" width="20" style="vertical-align:middle"></a>
										<a href="https://web.ecs.syr.edu/~ffiorett/cfp/PPAI22/29.mp4" alt="Video" style="border-bottom: none"><img src="images/play.png" width="20" style="vertical-align:middle"></a>
										<a href="https://arxiv.org/abs/2112.14087" alt="ArXiv Link" style="border-bottom: none"><strong>[ArXiv]</strong></a>
										<br>
										<em>Jiahao Lu (Institute of Automation, Chinese Academy of Sciences); Xi Sheryl Zhang (Institute of Automation, Chinese Academy of Sciences); Tianli Zhao (Institute of Automation,Chinese Academy of Sciences); Xiangyu He (Institute of Automation, Chinese Academy of Sciences); Jian Cheng (Chinese Academy of Sciences)</em>
									</li>
								</ul>
								
								<h5>Poster Presentations</h5>
								<ul>
									<li><font color="#f56a6a">Exploring the Unfairness of DP-SGD Across Settings</font>
										<a href="files/31.pdf" alt="ArXiv Link" style="border-bottom: none"><img src="images/pdf-icon.png" width="20" style="vertical-align:middle"></a>
										<a href="https://web.ecs.syr.edu/~ffiorett/cfp/PPAI22/31.mp4" alt="Video" style="border-bottom: none"><img src="images/play.png" width="20" style="vertical-align:middle"></a>
										<a href="https://arxiv.org/abs/2202.12058" alt="ArXiv Link" style="border-bottom: none"><strong>[ArXiv]</strong></a>
									<br>
									<em>Frederik A. Noe (The University of Copenhagen); Rasmus O.R. Herskind (University of Copenhagen); Anders Søgaard (University of Copenhagen)</em>
									</li>

									<li><font color="#f56a6a">Differentially Private Fractional Moments Estimation with Polylogarithmic Space</font>
									
									<a href="files/1.pdf" alt="ArXiv Link" style="border-bottom: none"><img src="images/pdf-icon.png" width="20" style="vertical-align:middle"></a>
									<a href="https://web.ecs.syr.edu/~ffiorett/cfp/PPAI22/1.mp4" alt="Video" style="border-bottom: none"><img src="images/play.png" width="20" style="vertical-align:middle"></a>
									<a href="https://arxiv.org/abs/2105.12363" alt="ArXiv Link" style="border-bottom: none"><strong>[ArXiv]</strong></a>
									<br>
									<em>Lun Wang (University of California, Berkeley); Iosif Pinelis (Michigan Technological University); Dawn Song (UC Berkeley)</em>
									</li>

									<li><font color="#f56a6a">Secure Federated Feature Selection</font>
									
									<a href="files/3.pdf" alt="ArXiv Link" style="border-bottom: none"><img src="images/pdf-icon.png" width="20" style="vertical-align:middle"></a>
									<a href="https://web.ecs.syr.edu/~ffiorett/cfp/PPAI22/3.mp4" alt="Video" style="border-bottom: none"><img src="images/play.png" width="20" style="vertical-align:middle"></a>
									<a href="https://arxiv.org/abs/2105.14618" alt="ArXiv Link" style="border-bottom: none"><strong>[ArXiv]</strong></a>
									<br>
									<em>Lun Wang (University of California, Berkeley); Qi Pang (HKUST); Shuai Wang (HKUST); Dawn Song (UC Berkeley)</em>
									</li>

									<li><font color="#f56a6a">Distributed Machine Learning and the Semblance of Trust</font>
									
									<a href="files/4.pdf" alt="ArXiv Link" style="border-bottom: none"><img src="images/pdf-icon.png" width="20" style="vertical-align:middle"></a>
									<a href="https://web.ecs.syr.edu/~ffiorett/cfp/PPAI22/4.mp4" alt="Video" style="border-bottom: none"><img src="images/play.png" width="20" style="vertical-align:middle"></a>
									<a href="https://arxiv.org/abs/2112.11040" alt="ArXiv Link" style="border-bottom: none"><strong>[ArXiv]</strong></a>
									<br>
									<em>Dmitrii Usynin (Imperial College London); Alexander Ziller (Technische Universität München); Daniel Rueckert (Imperial College London); Jonathan Passerat-Palmbach (Imperial College London / ConsenSys Health); Georgios Kaissis (Technische Universität München)</em>
									</li>

									<li><font color="#f56a6a">SDNist: Benchmark Data and Evaluation Tools for Data Synthesizers</font>
									
									<a href="files/7.pdf" alt="ArXiv Link" style="border-bottom: none"><img src="images/pdf-icon.png" width="20" style="vertical-align:middle"></a>
									<a href="https://web.ecs.syr.edu/~ffiorett/cfp/PPAI22/7.mp4" alt="Video" style="border-bottom: none"><img src="images/play.png" width="20" style="vertical-align:middle"></a>
									<br>
									<em>Grégoire Lothe (Sarus Technology); Christine Task (Knexus Research); Issac Slavitt (DrivenData, Inc.); Nicolas Grislain (Sarus); Karan Bhagat (Knexus Research); Gary Howarth (National Institue of Standards and Technology)</em>
									</li>

									<li><font color="#f56a6a">On privacy and confidentiality of communications in organizational graphs</font>
									<a href="files/8.pdf" alt="ArXiv Link" style="border-bottom: none"><img src="images/pdf-icon.png" width="20" style="vertical-align:middle"></a>
									<a href="https://web.ecs.syr.edu/~ffiorett/cfp/PPAI22/8.mp4" alt="Video" style="border-bottom: none"><img src="images/play.png" width="20" style="vertical-align:middle"></a>
									<a href="https://arxiv.org/abs/2105.13418" alt="ArXiv Link" style="border-bottom: none"><strong>[ArXiv]</strong></a>
									<br>
									<em>Masoumeh Shafieinejad (University of Waterloo); Robert Sim (Microsoft Research); Huseyin Inan (Microsoft Research ); Marcello Hasegawa (Microsoft)	</em>
									</li>

									<li><font color="#f56a6a">Privacy Preserving Visual Question Answering</font>
									<a href="files/10.pdf" alt="ArXiv Link" style="border-bottom: none"><img src="images/pdf-icon.png" width="20" style="vertical-align:middle"></a>
									<a href="https://web.ecs.syr.edu/~ffiorett/cfp/PPAI22/10.mp4" alt="Video" style="border-bottom: none"><img src="images/play.png" width="20" style="vertical-align:middle"></a>
									<a href="https://arxiv.org/abs/2202.07712" alt="ArXiv Link" style="border-bottom: none"><strong>[ArXiv]</strong></a>
									<br>
									<em>Cristian-Paul BARA (University of Michigan); Qing Ping (Amazon); Abhinav Mathur (Amazon); Rohith MV (Amazon Lab126); Govind Thattai (Amazon Alexa AI); Gaurav S Sukhatme (University of Southern California; Amazon)</em>
									</li>

									<li><font color="#f56a6a">LTU Attacker for Membership Inference</font>
									<a href="files/13.pdf" alt="ArXiv Link" style="border-bottom: none"><img src="images/pdf-icon.png" width="20" style="vertical-align:middle"></a>
									<a href="https://web.ecs.syr.edu/~ffiorett/cfp/PPAI22/13.mp4" alt="Video" style="border-bottom: none"><img src="images/play.png" width="20" style="vertical-align:middle"></a>
									<a href="https://arxiv.org/abs/2202.02278" alt="ArXiv Link" style="border-bottom: none"><strong>[ArXiv]</strong></a>
									<br>
									<em>Joseph Pedersen (Rensselaer Polytechnic Institute); Rafael Muñoz-Gómez (Université Paris-Saclay); Jiangnan Huang (Université Paris-Saclay); Haozhe Sun (Paris-Saclay University); Isabelle Guyon (CNRS, INRIA, University Paris-Saclay and ChaLearn); Wei-Wei Tu (4Paradigm Inc.)</em>
									</li>

									<li><font color="#f56a6a">Feature Space Hijacking Attacks against Differentially Private Split Learning</font>
									<a href="files/16.pdf" alt="ArXiv Link" style="border-bottom: none"><img src="images/pdf-icon.png" width="20" style="vertical-align:middle"></a>
									<a href="https://web.ecs.syr.edu/~ffiorett/cfp/PPAI22/16.mp4" alt="Video" style="border-bottom: none"><img src="images/play.png" width="20" style="vertical-align:middle"></a>
									<a href="https://arxiv.org/abs/2201.04018" alt="ArXiv Link" style="border-bottom: none"><strong>[ArXiv]</strong></a>
									<br>
									<em>Grzegorz Gawron (Liveramp); Philip Stubbings (LiveRamp)</em>
									</li>

									<li><font color="#f56a6a">BEAS: Blockchain Enabled Asynchronous & Secure Federated Machine Learning</font>
									<a href="files/20.pdf" alt="ArXiv Link" style="border-bottom: none"><img src="images/pdf-icon.png" width="20" style="vertical-align:middle"></a>
									<a href="https://web.ecs.syr.edu/~ffiorett/cfp/PPAI22/20.mp4" alt="Video" style="border-bottom: none"><img src="images/play.png" width="20" style="vertical-align:middle"></a>
									<a href="https://arxiv.org/abs/2202.02817" alt="ArXiv Link" style="border-bottom: none"><strong>[ArXiv]</strong></a>
									<br>
									<em>Arup Mondal (Ashoka University); Harpreet Virk (Ashoka University); Debayan Gupta (Ashoka University)</em>
									</li>

									<li><font color="#f56a6a">SCOTCH: An Efficient Secure Computation Framework for Secure Aggregation</font>
									<a href="files/21.pdf" alt="ArXiv Link" style="border-bottom: none"><img src="images/pdf-icon.png" width="20" style="vertical-align:middle"></a>
									<a href="https://web.ecs.syr.edu/~ffiorett/cfp/PPAI22/21.mp4" alt="Video" style="border-bottom: none"><img src="images/play.png" width="20" style="vertical-align:middle"></a>
									<a href="https://arxiv.org/abs/2201.07730" alt="ArXiv Link" style="border-bottom: none"><strong>[ArXiv]</strong></a>
									<br>
									<em>Arup Mondal (Ashoka University); Yash More (Ashoka University); Prashanthi Ramachandran (Brown University); Priyam Panda (Ashoka University); Harpreet Virk (Ashoka University); Debayan Gupta (Ashoka University)</em>
									</li>

									<li><font color="#f56a6a">Split HE: Fast Secure Inference Combining Split Learning and Homorphic Encryption</font>
									<a href="files/23.pdf" alt="ArXiv Link" style="border-bottom: none"><img src="images/pdf-icon.png" width="20" style="vertical-align:middle"></a>
									<a href="https://web.ecs.syr.edu/~ffiorett/cfp/PPAI22/23.mp4" alt="Video" style="border-bottom: none"><img src="images/play.png" width="20" style="vertical-align:middle"></a>
									<!-- <a href="https://arxiv.org/abs/" alt="ArXiv Link" style="border-bottom: none"><strong>[ArXiv]</strong></a> -->
									<br>
									<em>George Pereteanu (Imperial); Amir Alansary (Imperial College London); Jonathan Passerat-Palmbach (Imperial College London / ConsenSys Health)</em>
									</li>

									<li><font color="#f56a6a">PrivFair: a Library for Privacy-Preserving Fairness Auditing</font>
									<a href="files/24.pdf" alt="ArXiv Link" style="border-bottom: none"><img src="images/pdf-icon.png" width="20" style="vertical-align:middle"></a>
									<a href="https://web.ecs.syr.edu/~ffiorett/cfp/PPAI22/24.mp4" alt="Video" style="border-bottom: none"><img src="images/play.png" width="20" style="vertical-align:middle"></a>
									<a href="https://arxiv.org/abs/2202.04058" alt="ArXiv Link" style="border-bottom: none"><strong>[ArXiv]</strong></a>
									<br>
									<em>Sikha Pentyla (Mila - Quebec AI Institute); David Melanson (University of Washington, Tacoma); Martine De Cock (University of Washington Tacoma); Golnoosh Farnadi (Mila, Université de Montréal)</em>
									</li>

									<li><font color="#f56a6a">Anonymizing Trajectory Data: Limitations and Opportunities</font>
									<a href="files/25.pdf" alt="ArXiv Link" style="border-bottom: none"><img src="images/pdf-icon.png" width="20" style="vertical-align:middle"></a>
									<a href="https://web.ecs.syr.edu/~ffiorett/cfp/PPAI22/25.mp4" alt="Video" style="border-bottom: none"><img src="images/play.png" width="20" style="vertical-align:middle"></a>

									<br>
									<em>Patricia Guerra-Balboa (Karlsruhe Institute of Technology); Àlex Miranda Pascual (Universitat Politècnica de Catalunya); Javier Parra-Arnau (Karlsruhe Institute of Technology); Jordi Forné (Universitat Politècnica de Catalunya); Thorsten Strufe (Karlsruhe Institute of Technology)</em>
									</li>

									<li><font color="#f56a6a">DP-SGD vs PATE: Which Has Less Disparate Impact on GANs?</font>
									<a href="files/28.pdf" alt="ArXiv Link" style="border-bottom: none"><img src="images/pdf-icon.png" width="20" style="vertical-align:middle"></a>
									<a href="https://web.ecs.syr.edu/~ffiorett/cfp/PPAI22/28.mp4" alt="Video" style="border-bottom: none"><img src="images/play.png" width="20" style="vertical-align:middle"></a>
									<a href="https://arxiv.org/abs/2111.13617" alt="ArXiv Link" style="border-bottom: none"><strong>[ArXiv]</strong></a>
									<br>
									<em>Georgi Ganev (UCL)</em>
									</li>

									<li><font color="#f56a6a">Learning Privacy-Preserving Deep Kernels with Known Demographics</font>
									<a href="files/30.pdf" alt="ArXiv Link" style="border-bottom: none"><img src="images/pdf-icon.png" width="20" style="vertical-align:middle"></a>
									<a href="https://web.ecs.syr.edu/~ffiorett/cfp/PPAI22/30.mp4" alt="Video" style="border-bottom: none"><img src="images/play.png" width="20" style="vertical-align:middle"></a>

									<br>									
									<em>Namrata Deka (University of British Columbia); Danica J. Sutherland (University of British Columbia)</em>
									</li>

								</ul>


							<header class="major" id="tutorials">
								<h3>Tutorial</h3>
							</header>
							<h4 id="ilya"> 
							<strong>Differentially Private Deep Learning, Theory, Attacks, and PyTorch Implementation</strong>
							<!-- <a href="#" target="_blank"><img src="images/pdf-icon.png" width="20" style="vertical-align:middle" alt="slides"></a> -->
							<!-- <a href="#" target="_blank"><img src="images/play.png" width="20" style="vertical-align:middle" alt="video recording"></a> -->
							</h4>
							by <a href="https://ai.facebook.com/people/ilya-mironov/">Ilya Mironov</a> (Responsible AI, Meta),
							<a href="https://alexandresablayrolles.github.io/">Alexandre Sablayrolles</a> (Responsible AI, Meta), 
							and <a href="#">Igor Shilov</a> (Responsible AI, Meta)
								<p>
							<strong>Abstract</strong>: <br> 
							The tutorial is designed as a gentle introduction to the topic of differentially private deep learning. In the first part of the tutorial we cover relevant topics in the theory of differential privacy, such as composition theorems and privacy-preserving mechanisms. In the second part, we discuss Opacus, a PyTorch-based library for performant and user-friendly differentially private training, and Privacy Linter, a library for identifying privacy violations.
					        <span class="brmed"></span> 
							We begin by motivating privacy-preserving deep learning with several examples where industry-grade models demonstrably leak training data. We briefly review the notion of differential privacy (DP) as a remediation strategy, and learn how SGD-based optimization algorithms can be adapted to DP. We take a close look at several privacy accountants (upper bounds on privacy loss) and the complementary lower bounds.
					        <span class="brmed"></span> 
							In the second part of the tutorial we present our Pytorch framework for DP-SGD, Opacus. Opacus is designed to be a simple and extensible framework that can easily be plugged into an existing machine learning pipeline. Opacus’ features include implementations of several privacy accountants, vectorized computations of per-sample gradients, and support for distributed computations. We conclude by presenting the Privacy Linter, a PyTorch framework for evaluating practical privacy attacks on machine learning models.The Linter implements a number of recently proposed attacks on trained models and support for more advanced strategies.
							</p>
							

							<header class="major" id="invited_talks">
								<h3>Invited Talks</h3>
							</header>
							<h4 id="adam"> 
							<strong>When is Memorization Necessary for Machine Learning?</strong>
							<!-- <a href="#" target="_blank"><img src="images/pdf-icon.png" width="20" style="vertical-align:middle" alt="slides"></a> -->
							<!-- <a href="#" target="_blank"><img src="images/play.png" width="20" style="vertical-align:middle" alt="video recording"></a> -->
							</h4>
							by <a href="https://cs-people.bu.edu/ads22/index.html">Adam Smith</a> (Boston University)
							<p>
							<strong>Abstract</strong>: <br>
							Modern machine learning models are complex, and frequently encode surprising amounts of information about individual inputs. In extreme cases, complex models appear to memorize entire input examples, including seemingly irrelevant information (exact addresses from text, for example). In this talk, we aim to understand whether this sort of memorization is necessary for accurate learning, and what the implications are for privacy.
							<span class="brmed"></span> 
							We describe two results that explore different aspects of this phenomenon. In the first, published at STOC 2021, we give natural prediction problems in which every sufficiently accurate training algorithm must encode, in the prediction model, essentially all the information about a large subset of its training examples. This remains true even when the examples are high-dimensional and have entropy much higher than the sample size, and even when most of that information is ultimately irrelevant to the task at hand. Further, our results do not depend on the training algorithm or the class of models used for learning.
							<span class="brmed"></span> 
							Our second, unpublished result shows how memorization must occur during the training process, even when the final model is succinct and depends only on the underlying distribution. This leads to new lower bounds on the memory size of one-pass streaming algorithms for fitting natural models. 
							<span class="brmed"></span> 
							Joint work with (subsets of) Gavin Brown, Mark Bun, Vitaly Feldman, and Kunal Talwar.
							</p>

							<h4 id="claire">
							<strong>Personal Privacy and the Public Good</strong>
							<!-- <a href="#" target="_blank"><img src="images/pdf-icon.png" width="20" style="vertical-align:middle" alt="slides"></a> -->
							<!-- <a href="#" target="_blank"><img src="images/play.png" width="20" style="vertical-align:middle" alt="video recording"></a> -->
							</h4>
							by <a href="https://www.clairemckaybowen.com/">Claire McKay Bowen</a> (Urban Institute)
							<p>
							<strong>Abstract</strong>: <br> 
							Both privacy experts and policymakers are at an impasse, trying to answer,
							<span class="brmed"></span> 
							“At what point does the sacrifice to our personal information outweigh the public good?” 
							<span class="brmed"></span> 
							If public policymakers had access to society’s personal and confidential data, they could make more evidence-based, data-informed decisions that could accelerate economic recovery and improve COVID-19 vaccine distribution. Although privacy researchers strive to balance the need for data privacy and accuracy, access to personal data comes at a steep privacy cost for contributors, especially underrepresented groups. This situation results in many federal statistical agencies in the United States to never produce public data or restrict the data to a select few external researchers. Further, most public data users and policymakers are not familiar with data privacy and confidentiality methods and must make informed policy decisions on how to best balance the need for confidential data access and privacy protection. 
 							<span class="brmed"></span> 
 							This talk will cover several projects conducted at the intersection of expanding access to confidential data and public policy, the lessons learned when working with data users and non-privacy researchers, and the inequity of data privacy and confidentiality methodologies for underrepresented groups. 
							</p>

							<h4 id="damien">
							A bottom-up approach to making differential privacy ubiquitous
							<!-- <a href="#" target="_blank"><img src="images/pdf-icon.png" width="20" style="vertical-align:middle" alt="slides"></a> -->
							</h4>
							by <a href="https://desfontain.es/serious.html">Damien Desfontaines</a> (Tumult Labs) 
							<p>	
							<strong>Abstract</strong>: <br> 
							Among computer science researchers, differential privacy has been the
							gold standard for anonymization for over a decade. The real world is
							starting to catch up, albeit slowly. There is a need for strong
							anonymization techniques in small and large organizations alike… but
							it seems like only large organizations end up deploying DP for
							practical use cases, and only because they can afford to invest in
							specialized science and engineering teams to help them. How can we
							bridge this gap, and drive the widespread adoption of differential
							privacy?
							<span class="brmed"></span> 
							This talk will outline a bottom-up approach to bring differential
							privacy to a much more widespread audience. From initial outreach
							efforts all the way to production deployments, I will describe what a
							compelling solution could look like, and what role the scientific
							community can play in these efforts.
							</p>

							<header class="major" id="panel">
								<h2>PPAI-22 Panel:</h2>
								<h3>Differential Privacy and disparate impacts in downstream decisions and learning tasks</h3>
							</header>
							
							<h3>Panelists</h3>

							<div class="row">
								<div class="col-4 col-12-medium">
									<span class="image left"><a href="https://people.cs.umass.edu/~miklau/"><img class="center" src="images/gerome.jpeg" alt=""/></a></span>
									<div class="content"><p class="name">Gerome Miklau</p>University of Massachusetts, Amherst and Tumult Labs<br><br>
									</div>
								</div>

								<div class="col-4 col-12-medium">
									<span class="image left"><a href="https://www.clairemckaybowen.com"><img class="center" src="images/claire.jpg" alt=""/></a></span>
									<div class="content"><p class="name">Claire McKay Bowen</p>Urban Institute<br><br>
									</div>
								</div>

								<div class="col-4 col-12-medium">
									<span class="image left"><a href="https://blogs.cornell.edu/abowd/"><img class="center" src="images/JohnAbowd.jpg" alt=""/></a></span>
									<div class="content"><p class="name">John M. Abowd</p>U.S. Census Bureau<br><br>
									</div>
								</div>

								<div class="col-4 col-12-medium">
									<span class="image left"><a href="https://zstevenwu.com/"><img class="center" src="images/ZSW.jpeg" alt=""/></a></span>
									<div class="content"><p class="name">Steven Wu</p>Carnegie Mellon University<br><br>
									</div>
								</div>

	
 					</section>

						<section>
							<header class="major" id="invited">
								<h2>Invited Speakers</h2>
							</header>

							<div class="row">
								<div class="col-4 col-12-medium">
									<span class="image left"><a href="https://cs-people.bu.edu/ads22/index.html"><img class="center" src="images/adam.jpg" alt=""/></a></span>
									<div class="content"><p class="name">Adam Smith</p>Boston University<br><br>
										<p><a href="#adam">Talk details</a></p>
									</div>
								</div>

								<div class="col-4 col-12-medium">
									<span class="image left"><a href="https://www.clairemckaybowen.com"><img class="center" src="images/claire.jpg" alt=""/></a></span>
									<div class="content"><p class="name">Claire McKay Bowen</p>Urban Institute<br><br>
										<p><a href="#claire">Talk details</a></p>
									</div>
								</div>

								<div class="col-4 col-12-medium">
									<span class="image left"><a href="https://desfontain.es/serious.html"><img class="center" src="images/damien.jpeg" alt=""/></a></span>
									<div class="content"><p class="name">Damien Desfontaines</p>Tumult Labs<br><br>
										<p><a href="#damien">Talk details</a></p>
									</div>
								</div>

								<div class="col-4 col-12-medium">
									<span class="image left"><a href="https://ai.facebook.com/people/ilya-mironov/"><img class="center" src="images/ilya.jpg" alt=""/></a></span>
									<div class="content"><p class="name">Ilya Mironov</p>Responsible AI (Meta)<br><br>
										<p><a href="#ilya">Tutorial details</a></p>
									</div>
								</div>

							</div>
						</section>

						<section>
							<header class="major" id="pc">
								<h2>Program Committee</h2>
							</header>
							<ul>
								<li>Amrita		Roy Chowdhury		    (University of Wisconsin-Madison)</li>
								<li>Aurélien		Bellet			    (INRIA)</li>
								<li>Carsten			Baum			    (Aarhus University)</li>
								<li>Catuscia		Palamidessi		    (Laboratoire d'informatique de l'École polytechnique)</li>
								<li>Christine		Task			    (Knexus Research)</li>
								<li>Cuong			Tran			    (Syracuse University)</li>
								<li>Di				Wang			    (KAUST)</li>
								<li>Elette			Boyle			    (IDC H)</li>
								<li>Fatemeh			Mireshghallah		(University of California, San Diego)</li>
								<li>Graham			Cormode			    (University of Warwick)</li>
								<li>Hanieh			Hashemi			    (University of Southern California)</li>
								<li>Hao				Wang			    (Rutgers University)</li>
								<li>Ivan			Habernal			(Technical University of Darmstadt)</li>
								<li>Jan				Ramon			    (INRIA, FR)</li>
								<li>Jianfeng		Chi				    (University of Virginia)</li>
								<li>Keyu			Zhu					(Georgia Tech)</li>
								<li>Kobbi			Nissim			    (Georgetown University)</li>
								<li>Marco			Romanelli		    (CentraleSupélec - CNRS - L2S )</li>
								<li>Mark			Bun				    (Boston University)</li>
								<li>Michael			Hay				    (Colgate University)</li>
								<li>Mohamed 	Ali	Kaafar			    (Macquarie University and CSIRO-Data61)</li>
								<li>Mohammad Mahdi	Khalili			    (University of Delaware)</li>
								<li>Olga			Ohrimenko		    (The University of Melbourne)</li>
								<li>Paritosh	P	Ramanan			    (Georgia Institute of Technology)</li>
								<li>Rakshit			Naidu			    (Carnegie Mellon University)</li>
								<li>Ranya			Aloufi			    (Imperial College London)</li>
								<li>Raouf			Kerkouche		    (CISPA – Helmholtz Center for Information Security)</li>
								<li>Santiago 		Zanella-Beguelin    (Microsoft Research)</li>
								<li>Terrence WK		Mak				    (Georgia Institute of Technology)</li>
								<li>Vikrant			Singhal			    (Northeastern University )</li>
								<li>Xi				He				    (University of Waterloo)</li>
								<!-- <li>Yuliia			Lut				    (Columbia University)</li> -->
								<li>Yunwen			Lei				    (University of Birmingham)</li>
								<li>Zhiqi			Bu				    (University of Pennsylvania)</li>
							</ul>
						</section>

						<section>
							<header class="major" id="chairs">
								<h2>Workshop Chairs</h2>
							</header>

							<div class="row">
								<!-- <article> -->
								<div class="col-4 col-12-medium">
									<span class="image left">
										<a href="https://www.nandofioretto.com"><img class="center" src="images/Nando.png" alt=""/></a>
									</span>
									<div class="content">
										<p class="name">Ferdinando Fioretto</p>
										Syracuse University
										<br><br>
										<p>
											<a href="mailto:ffiorett@syr.edu">ffiorett@syr.edu</a>
										</p>
									</div>
								</div>
								<!-- <article> -->
								<div class="col-4 col-12-medium">
									<span class="image left"><a href="https://viterbi.usc.edu/directory/faculty/Korolova/Aleksandra">
										<img class="center" src="images/Alexsandra.jpg" alt=""/></a></span>
									<div class="content">
										<p class="name">Aleksandra Korolova</p>
										University of Southern California
										<br><br>
										<p>
											<a href="mailto:korolova@usc.edu">korolova@usc.edu</a>
										</p>
									</div>
								</div>
								<!-- <article> -->
								<div class="col-4 col-12-medium">
									<span class="image left"><a href="https://sites.gatech.edu/pascal-van-hentenryck/"><img class="center" src="images/Pascal.png" alt=""/></a></span>
									<div class="content">
										<p class="name">Pascal Van Hentenryck</p>
										Georgia Institute of Technology
										<br><br>
										<p>
											<a href="mailto:pvh@isye.gatech.edu">pvh@isye.gatech.edu</a>
										</p>
									</div>
								</div>
								<!-- </article> -->
							</div>
						</section>
				</div>
			</div>

		<!-- Sidebar -->
			<div id="sidebar">
				<div class="inner">
					<!-- Menu -->
						<nav id="menu">
							<header class="major">
								<h2>PPAI22</h2>
							</header>
							<ul>
								<li><a href="#scope">Scope</a></li>
								<li><a href="#dates">Important Dates</a></li>
								<li><a href="#submission">Submission</a></li>
								<li>
									<span class="opener">Program</span>
									<ul>
									<li><a href="#program">Schedule</a></li>
									<li><a href="#tutorials">Tutorials</a></li>
									<li><a href="#invited_talks">Invited Talks</a></li>
									<li><a href="#panel">Panel</a></li>
									</ul>
								</li>
                                <li><a href="#registration">Registration</a></li>
								<li><a href="#accepted_papers">Accepted Papers</a></li>
								<li><a href="#invited">Invited Speakers</a></li> 
 								<li><a href="#pc">Program Committee</a></li>
								<li><a href="#chairs">Workshop Chairs</a></li>
								<li>
									<span class="opener">Previous Editions</span>
									<ul>
									<li><a href="https://ppai21.github.io/">PPAI 2021</a></li>
									<li><a href="https://www2.isye.gatech.edu/~fferdinando3/cfp/PPAI20/">PPAI 2020</a></li>
									</ul>
								</li>

							</ul>
						</nav>

					<!-- Section -->
						<section>
							<header class="major">
								<h2>Contacts</h2>
							</header>
<!-- 									<p>.</p> -->	
						<ul class="contact">
							<li class="icon solid fa-envelope"><a href="mailto:ffiorett@syr.edu,">Email chairs</a></li>
							<!-- <li class="icon brands fa-twitter"><a href="https://twitter.com/PPAI211">Twitter</a></li> -->
						</ul>
						</section>

						<!-- Footer -->
							<footer id="footer">
								<p class="copyright">Website template from: <a href="https://html5up.net">HTML5 UP</a>.</p>
							</footer>

					</div>
				</div>

		</div>

	<!-- Scripts -->
		<script src="assets/js/jquery.min.js"></script>
		<script src="assets/js/browser.min.js"></script>
		<script src="assets/js/breakpoints.min.js"></script>
		<script src="assets/js/util.js"></script>
		<script src="assets/js/main.js"></script>

	</body>
</html>
